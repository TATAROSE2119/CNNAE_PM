import torch


def check_torch_gpu():
    print("========== PyTorch ç¯å¢ƒæ£€æµ‹ ==========")

    # 1. æ£€æŸ¥ PyTorch æ˜¯å¦å®‰è£…
    try:
        print(f"âœ… PyTorch ç‰ˆæœ¬ï¼š{torch.__version__}")
    except Exception as e:
        print("âŒ æœªæ£€æµ‹åˆ° PyTorchï¼Œè¯·å…ˆå®‰è£…ï¼špip install torch")
        return

    # 2. æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    print(f"ğŸ’¡ CUDA å¯ç”¨ï¼š{cuda_available}")

    if not cuda_available:
        # æä¾›æ’æŸ¥å»ºè®®
        print("âš ï¸ æ£€æµ‹åˆ° CUDA ä¸å¯ç”¨ï¼Œå¯èƒ½åŸå› ï¼š")
        print("  1. æœªå®‰è£… CUDA Toolkit æˆ–ç‰ˆæœ¬ä¸åŒ¹é…ã€‚")
        print("  2. GPU é©±åŠ¨æœªå®‰è£…æˆ–ç‰ˆæœ¬è¿‡ä½ã€‚")
        print("  3. å½“å‰ PyTorch æ˜¯ CPU ç‰ˆæœ¬ï¼ˆéœ€å®‰è£…å¸¦ cuda çš„ç‰ˆæœ¬ï¼‰ã€‚")
        print("  4. ä½¿ç”¨çš„æ˜¯è™šæ‹Ÿæœº/WSL æœªå¯ç”¨ GPU ç›´é€šã€‚")
        return

    # 3. æ£€æŸ¥ GPU ä¿¡æ¯
    device_count = torch.cuda.device_count()
    print(f"ğŸ–¥ï¸ æ£€æµ‹åˆ° GPU æ•°é‡ï¼š{device_count}")
    for i in range(device_count):
        print(f"  [{i}] {torch.cuda.get_device_name(i)}")
        print(f"     å½“å‰æ˜¾å­˜å ç”¨ï¼š{torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MB")
        print(f"     å¯ç”¨æ˜¾å­˜ï¼š{torch.cuda.get_device_properties(i).total_memory / 1024 ** 3:.2f} GB")

    # 4. æ£€æŸ¥å½“å‰è®¾å¤‡
    current_device = torch.cuda.current_device()
    print(f"ğŸ¯ å½“å‰é»˜è®¤è®¾å¤‡ï¼š{torch.cuda.get_device_name(current_device)}")
    print("=====================================")


if __name__ == "__main__":
    check_torch_gpu()
